{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DYNAMIC Peak count and depolarization start times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v3 changelist: \n",
    "- Fixed depolarization start detection (robustly)!\n",
    "\n",
    "TO DO:\n",
    "\n",
    "- Add spontaneous peak measurement function (paced at 0.5Hz, expect beat every 2sec, all others should be counted as n spontaneous)\n",
    "\n",
    "# QUESTIONS?: huynh.trung@mayo.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "# Please update this path to your actual file path\n",
    "excel_path = r\"C:\\Users\\m254292\\Downloads\\8112025 Results Excel.xlsx\"\n",
    "\n",
    "\n",
    "band_factor     = 1.5 ##default is 1.5, increasing = wider detection, decreasing = reduce noise\n",
    "upstroke_min    = 0.06 ##default is 0.6, increasing = more stringent peak detection\n",
    "\n",
    "APPLY_FILTERS        = True    # toggle filters\n",
    "GREY_OUT_UNFILTERED  = True    # grey removed events on plots\n",
    "\n",
    "xls = pd.ExcelFile(excel_path)\n",
    "\n",
    "def analyze_roi_signal(time, signal, sample_name, roi_label,\n",
    "                       apply_filters=True,\n",
    "                       grey_out_unfiltered=True):\n",
    "    # --- Prep ---\n",
    "    signal = pd.to_numeric(signal, errors='coerce').dropna().values\n",
    "    time   = time[: len(signal)]\n",
    "\n",
    "    Fmin       = np.min(signal)\n",
    "    normalized = (signal - Fmin) / Fmin\n",
    "    smoothed   = gaussian_filter1d(normalized, sigma=1)\n",
    "\n",
    "    slope_initial = np.gradient(smoothed, time)\n",
    "\n",
    "    # Photobleaching correction\n",
    "    flat_thresh = np.percentile(np.abs(slope_initial), 20)\n",
    "    flat_idxs   = np.where(np.abs(slope_initial) < flat_thresh)[0]\n",
    "    if flat_idxs.size >= 2:\n",
    "        bleach_curve = np.interp(time, time[flat_idxs], smoothed[flat_idxs])\n",
    "        bleach_curve[bleach_curve == 0] = 1\n",
    "        smoothed = smoothed / bleach_curve\n",
    "\n",
    "    # Thresholds\n",
    "    s_max, s_med = np.max(smoothed), np.median(smoothed)\n",
    "    dr            = s_max - s_med\n",
    "    rising_thresh = 0.05 * dr ## Default = 0.05. Increase if noisy peaks\n",
    "    depol_thresh  = 0.02 * dr  ## Default = 0.02. Marks transition point from flat baseline to beginning of depol upstroke. Lower = depol earlier\n",
    "\n",
    "    slope = np.gradient(smoothed, time)\n",
    "\n",
    "    # Rising-edge groups\n",
    "    edges  = np.where(slope > rising_thresh)[0]\n",
    "    groups = []\n",
    "    for k, g in groupby(enumerate(edges), lambda x: x[0] - x[1]):\n",
    "        grp = [i for _, i in g]\n",
    "        if len(grp) >= 5:\n",
    "            groups.append(grp)\n",
    "\n",
    "    # Candidate peaks\n",
    "    peeks = []\n",
    "    for grp in groups:\n",
    "        start = grp[0]\n",
    "        end   = min(grp[-1] + 20, len(smoothed))\n",
    "        peeks.append(np.argmax(smoothed[start:end]) + start)\n",
    "\n",
    "    # De-dupe\n",
    "    filt_peeks = []\n",
    "    for p in peeks:\n",
    "        if not filt_peeks or p - filt_peeks[-1] > 40:\n",
    "            filt_peeks.append(p)\n",
    "\n",
    "    # Metrics\n",
    "    rows = []\n",
    "    for pk in filt_peeks:\n",
    "        # --- START OF UPDATED LOGIC ---\n",
    "\n",
    "        # 1. Define a window to find the max upstroke velocity (Vmax)\n",
    "        #    This window looks at the 100 points leading up to the peak.\n",
    "        vmax_search_window = range(max(0, pk - 50), pk + 1)\n",
    "        if not list(vmax_search_window):\n",
    "            continue\n",
    "\n",
    "        # 2. Find the index of the actual Vmax within that window\n",
    "        #    This identifies the steepest part of the depolarization.\n",
    "        vmax_local_idx = np.argmax(slope[vmax_search_window])\n",
    "        vmax_idx = vmax_search_window[vmax_local_idx]\n",
    "\n",
    "        # 3. Define a *new* search window for the depolarization start.\n",
    "        #    This window ends right before the Vmax point, effectively ignoring any notches near the peak.\n",
    "        start_search_window = range(max(0, pk - 50), vmax_idx)\n",
    "\n",
    "        # 4. Find the last point where the slope is below the threshold within this SAFER window.\n",
    "        cand = [i for i in start_search_window if slope[i] < depol_thresh]\n",
    "\n",
    "        if not cand:\n",
    "            # Fallback in case no clear flat baseline is found before Vmax.\n",
    "            # This can happen with noisy signals. We'll find the minimum value\n",
    "            # in the window before Vmax as a robust estimate of the start.\n",
    "            if not list(start_search_window):\n",
    "                continue\n",
    "            min_val_local_idx = np.argmin(smoothed[start_search_window])\n",
    "            start_idx = start_search_window[min_val_local_idx]\n",
    "        else:\n",
    "            # This is the ideal case: we found the last point of the flat baseline.\n",
    "            start_idx = cand[-1]\n",
    "\n",
    "        # --- END OF UPDATED LOGIC ---\n",
    "\n",
    "        pre       = smoothed[max(0, start_idx - 50): start_idx] ##originally 50 data points (for baseline calculation)\n",
    "        baseline  = np.min(pre)\n",
    "        peak_val  = smoothed[pk]\n",
    "        amp       = peak_val - baseline\n",
    "        vmax      = np.max(slope[start_idx:pk])\n",
    "\n",
    "\n",
    "        # CTD90\n",
    "        lvl90 = baseline + 0.1 * amp\n",
    "        repol90_idx = next((i for i in range(pk + 1, len(smoothed)) if smoothed[i] <= lvl90), None)\n",
    "        if repol90_idx is None:\n",
    "            continue\n",
    "        repol90_time  = time[repol90_idx]\n",
    "        repol90_level = smoothed[repol90_idx]\n",
    "        ctd90         = repol90_time - time[start_idx]\n",
    "        peak_to_90_decay = repol90_time - time[pk]\n",
    "\n",
    "\n",
    "        # CTD50\n",
    "        lvl50 = baseline + 0.5 * amp\n",
    "        repol50_idx = next((i for i in range(pk + 1, len(smoothed)) if smoothed[i] <= lvl50), None)\n",
    "        if repol50_idx is None:\n",
    "            continue\n",
    "        repol50_time = time[repol50_idx]\n",
    "        ctd50 = repol50_time - time[start_idx]\n",
    "        peak_to_50_decay = repol50_time - time[pk]\n",
    "\n",
    "\n",
    "        ratio50_90 = ctd50 / ctd90 if ctd90 > 0 else np.nan\n",
    "        upstroke   = time[pk] - time[start_idx]\n",
    "\n",
    "        rows.append({\n",
    "            'Depolarization_Start_Time_s': time[start_idx],\n",
    "            'Peak_Time_s'                : time[pk],\n",
    "            'Amplitude'                  : amp,\n",
    "            'CTD50_s'                    : ctd50,\n",
    "            'CTD90_s'                    : ctd90,\n",
    "            'Peak_to_50%_Decay_Time_s'    : peak_to_50_decay,\n",
    "            'Peak_to_90%_Decay_Time_s'    : peak_to_90_decay,\n",
    "            'Upstroke_Time_s'            : upstroke,\n",
    "            'Vmax'                       : vmax,\n",
    "            'Diastolic_Calcium'          : baseline\n",
    "        })\n",
    "\n",
    "    # DataFrame\n",
    "    df_res_raw = pd.DataFrame(rows).reset_index().rename(columns={'index': '_orig'})\n",
    "    df_res     = df_res_raw.copy()\n",
    "\n",
    "    # --- Filters ---\n",
    "    if apply_filters and not df_res.empty:\n",
    "        # Upstroke filter\n",
    "        df_res = df_res[df_res['Upstroke_Time_s'] > upstroke_min]\n",
    "\n",
    "        # Symmetric APD90 trim\n",
    "        median_ctd90 = df_res['CTD90_s'].median()\n",
    "        lower = median_ctd90 / band_factor\n",
    "        upper = median_ctd90 * band_factor\n",
    "        df_res = df_res[(df_res['CTD90_s'] >= lower) & (df_res['CTD90_s'] <= upper)]\n",
    "\n",
    "        df_res = df_res.reset_index(drop=True)\n",
    "\n",
    "    # Figure out which events were removed (only for plotting, not for summary printing)\n",
    "    removed = (df_res_raw[~df_res_raw['_orig'].isin(df_res['_orig'])]\n",
    "               if apply_filters and grey_out_unfiltered else\n",
    "               pd.DataFrame(columns=df_res_raw.columns))\n",
    "\n",
    "    expected = [\n",
    "        'Depolarization_Start_Time_s', 'Peak_Time_s', 'Amplitude',\n",
    "        'CTD50_s', 'CTD90_s', 'Peak_to_50%_Decay_Time_s', 'Peak_to_90%_Decay_Time_s', 'Upstroke_Time_s',\n",
    "        'Vmax', 'Diastolic_Calcium'\n",
    "    ]\n",
    "    df_res = df_res.reindex(columns=['_orig'] + expected)\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(time, smoothed, label='Signal')\n",
    "\n",
    "    # Grey-out removed (if desired)\n",
    "    if grey_out_unfiltered and not removed.empty:\n",
    "        for i, t in enumerate(removed['Peak_Time_s']):\n",
    "            ax.axvline(t, color='grey', linestyle='--', alpha=0.3,\n",
    "                       label='Removed Peak' if i == 0 else None)\n",
    "        for i, t in enumerate(removed['Depolarization_Start_Time_s']):\n",
    "            ax.axvline(t, color='grey', linestyle=':', alpha=0.3,\n",
    "                       label='Removed Depol Start' if i == 0 else None)\n",
    "\n",
    "    # Kept events\n",
    "    for i, t in enumerate(df_res['Peak_Time_s']):\n",
    "        ax.axvline(t, color='green', linestyle='--', alpha=0.8,\n",
    "                   label='Peak' if i == 0 else None)\n",
    "    for i, t in enumerate(df_res['Depolarization_Start_Time_s']):\n",
    "        ax.axvline(t, color='black', linestyle='--', alpha=0.5,\n",
    "                   label='Depol Start' if i == 0 else None)\n",
    "\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"ΔF/Fmin\")\n",
    "    ax.set_title(f\"{sample_name} | {roi_label} (bleach-corrected)\")\n",
    "    \n",
    "    # Set x-axis ticks to 1-second increments\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    \n",
    "    ax.grid(True)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return sample_name, roi_label, df_res, fig\n",
    "\n",
    "# === Main ===\n",
    "all_results = []\n",
    "plots       = []\n",
    "\n",
    "for sheet_name in xls.sheet_names:\n",
    "    raw = xls.parse(sheet_name, header=None, nrows=1)\n",
    "    if raw.dropna(how='all').empty:\n",
    "        continue\n",
    "    sample_name = str(raw.iloc[0, 0])\n",
    "    if sample_name.lower().endswith('.nd2'):\n",
    "        sample_name = sample_name[:-4]\n",
    "\n",
    "    df = xls.parse(sheet_name, header=1)\n",
    "    if df.dropna(how='all').empty:\n",
    "        continue\n",
    "    df.columns = df.columns.astype(str).str.strip()\n",
    "\n",
    "    time_cols = [c for c in df.columns if 'time' in c.lower()]\n",
    "    if not time_cols:\n",
    "        continue\n",
    "    time = pd.to_numeric(df[time_cols[0]], errors='coerce').values\n",
    "\n",
    "    roi_cols = [c for c in df.columns if c != time_cols[0] and 'Mono' in c]\n",
    "    for roi in roi_cols:\n",
    "        samp, roi_label, res_df, fig = analyze_roi_signal(\n",
    "            time, df[roi], sample_name, roi,\n",
    "            apply_filters=APPLY_FILTERS,\n",
    "            grey_out_unfiltered=GREY_OUT_UNFILTERED\n",
    "        )\n",
    "\n",
    "        res_df.insert(0, 'Sample', samp)\n",
    "        res_df.insert(1, 'ROI', roi_label)\n",
    "\n",
    "        all_results.append(res_df)\n",
    "        plots.append((samp, roi_label, fig))\n",
    "\n",
    "        # === FIX: Display the plot immediately after creation ===\n",
    "        print(f\"Displaying plot for: {samp} | {roi_label}\")\n",
    "        display(fig)\n",
    "        plt.close(fig) # Prevents the figure from being held in memory and displayed again later\n",
    "\n",
    "\n",
    "\n",
    "# === Only filtered summary ===\n",
    "if all_results:\n",
    "    summary_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # === UPDATED: Sort the summary_df by Sample and numerical ROI ===\n",
    "    summary_df['ROI_num'] = summary_df['ROI'].str.extract('(\\d+)').astype(int)\n",
    "    summary_df = summary_df.sort_values(by=['Sample', 'ROI_num']).drop(columns=['ROI_num']).reset_index(drop=True)\n",
    "\n",
    "    print(\"Filtered summary:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(summary_df)\n",
    "\n",
    "    # === Calculate mean and standard deviation values per ROI ===\n",
    "    mean_summary_df = summary_df.groupby(['Sample', 'ROI'])[[\n",
    "        'Amplitude',\n",
    "        'CTD50_s',\n",
    "        'CTD90_s',\n",
    "        'Peak_to_50%_Decay_Time_s',\n",
    "        'Peak_to_90%_Decay_Time_s',\n",
    "        'Upstroke_Time_s',\n",
    "        'Vmax',\n",
    "        'Diastolic_Calcium'\n",
    "    ]].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Flatten the column names after aggregation\n",
    "    mean_summary_df.columns = ['_'.join(col).strip() for col in mean_summary_df.columns.values]\n",
    "    mean_summary_df = mean_summary_df.rename(columns={'Sample_': 'Sample', 'ROI_': 'ROI'})\n",
    "\n",
    "\n",
    "    # === UPDATED: Sort the mean_summary_df by Sample and numerical ROI ===\n",
    "    mean_summary_df['ROI_num'] = mean_summary_df['ROI'].str.extract('(\\d+)').astype(int)\n",
    "    mean_summary_df = mean_summary_df.sort_values(by=['Sample', 'ROI_num']).drop(columns=['ROI_num']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# === ADDED: Display the DataFrames ===\n",
    "print(\"Filtered summary:\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(summary_df)\n",
    "\n",
    "print(\"\\nMean values per ROI:\")\n",
    "display(mean_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dynamically build out_path using os.path ===\n",
    "folder, basename = os.path.split(excel_path)\n",
    "name, ext       = os.path.splitext(basename)\n",
    "new_name        = f\"{name} batch analysis{ext}\"\n",
    "out_path        = os.path.join(folder, new_name)\n",
    "\n",
    "# === ADDED: Sort the plots list to match the summary sheet order ===\n",
    "# This will sort the plots first by sample name, then by the numerical ROI number\n",
    "plots.sort(key=lambda x: (x[0], int(re.search(r'#(\\d+)', x[1]).group(1))))\n",
    "\n",
    "\n",
    "# === Write summary_df, mean_summary_df, and plots into the new workbook ===\n",
    "with pd.ExcelWriter(out_path, engine='xlsxwriter') as writer:\n",
    "    # 1) summary table\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "    # 2) mean values table\n",
    "    mean_summary_df.to_excel(writer, sheet_name='Mean_Values', index=False)\n",
    "\n",
    "    # 3) embed plots\n",
    "    workbook  = writer.book\n",
    "    worksheet = workbook.add_worksheet('Plots')\n",
    "\n",
    "    img_row = 0\n",
    "    for sample, roi, fig in plots:\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "\n",
    "        worksheet.write(img_row, 0, f\"{sample} | {roi}\")\n",
    "        img_row += 1\n",
    "\n",
    "        worksheet.insert_image(\n",
    "            img_row, 0,\n",
    "            f\"{sample}_{roi}.png\",\n",
    "            {'image_data': buf, 'x_scale': 0.8, 'y_scale': 0.8}\n",
    "        )\n",
    "        img_row += 30\n",
    "\n",
    "print(f\"\\n✅ Saved batch analysis to:\\n  {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: \n",
    "- Add APD50\n",
    "- Add ratio of APD50/90\n",
    "https://github.com/trunghuynh25/APD-VSC/blob/2fc292274f67dec9940add6c963d6449c9ae6adf/APD%20v6b.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
